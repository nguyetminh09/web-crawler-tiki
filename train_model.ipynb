{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_crawler.csv', delimiter='\\t', header=None)\n",
    "print(df.shape)\n",
    "# get all rows\n",
    "# print(df[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode [101, 2089, 14684, 2050, 5926, 7570, 4017, 25945, 7658, 2232, 1010, 18699, 2319, 2089, 20934, 10448, 6369, 16215, 2072, 20934, 10448, 9610, 13765, 18699, 2319, 9543, 18699, 2319, 5926, 7570, 4017, 25945, 7658, 2232, 1102, 9013, 10722, 1012, 25175, 18699, 2319, 2089, 16215, 2072, 15740, 24110, 12849, 3158, 1102, 2063, 21025, 1010, 1102, 2063, 16480, 1060, 6633, 10514, 29328, 2068, 27793, 2072, 27699, 2078, 1060, 6633, 7509, 1012, 1012, 1012, 1012, 2522, 1102, 17301, 1102, 5063, 2175, 2072, 12849, 8802, 4958, 2000, 2102, 1010, 9610, 1051, 14163, 2278, 17214, 15775, 2361, 18699, 2319, 29536, 2072, 12256, 6865, 13012, 27699, 25957, 2322, 16344, 1012, 12436, 2522, 1102, 17301, 22794, 2278, 6097, 2474, 13843, 7509, 2089, 25175, 21110, 12849, 2522, 7744, 6154, 1006, 8840, 4886, 8945, 2278, 9152, 7811, 25945, 24110, 2232, 6154, 1007, 11265, 2078, 17214, 16480, 1018, 1008, 2310, 11834, 11320, 5063, 4487, 2818, 24728, 1010, 2310, 2089, 16215, 2072, 14684, 2050, 1996, 18699, 2319, 1060, 3388, 21025, 18699, 17301, 102]\n",
      "decode [CLS] may chua kick hoat bao hanh, nhan may buoi sang thi buoi chieu nhan tin nhan kick hoat bao hanh đien tu. moi nhan may thi tong quan ko van đe gi, đe cho xem su dung them thoi gian xem sao.... co đieu đong goi ko chen ep tot, chi o muc tam chap nhan voi mon hang tri gia gan 20tr. va co đieu thac mac la tai sao may moi lai ko co seal hop ( loai boc nilon bao quanh hop ) nen tam cho 4 * ve chat luong dich vu, ve may thi chua the nhan xet gi nhieu [SEP]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load pretrain model/ tokenizers\n",
    "'''\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#encode lines\n",
    "tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens = True)))\n",
    "print('encode',tokenized[1])\n",
    "# decode\n",
    "print('decode',tokenizer.decode(tokenized[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning model and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '?'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DE\\Desktop\\Crawler\\crawler\\train_model.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DE/Desktop/Crawler/crawler/train_model.ipynb#ch0000006?line=1'>2</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(df[\u001b[39m0\u001b[39m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DE/Desktop/Crawler/crawler/train_model.ipynb#ch0000006?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df[\u001b[39m0\u001b[39m])):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DE/Desktop/Crawler/crawler/train_model.ipynb#ch0000006?line=3'>4</a>\u001b[0m     labels[i] \u001b[39m=\u001b[39m df[\u001b[39m0\u001b[39m][i][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DE/Desktop/Crawler/crawler/train_model.ipynb#ch0000006?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlabels shape:\u001b[39m\u001b[39m'\u001b[39m, labels\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DE/Desktop/Crawler/crawler/train_model.ipynb#ch0000006?line=6'>7</a>\u001b[0m \u001b[39m# get lenght max of tokenized\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '?'"
     ]
    }
   ],
   "source": [
    "#get all label \n",
    "labels = np.zeros(len(df[0]))\n",
    "for i in range(len(df[0])):\n",
    "    labels[i] = df[0][i][-1]\n",
    "print('labels shape:', labels.shape)\n",
    "\n",
    "# get lenght max of tokenized\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print('max len:', max_len)\n",
    "\n",
    "# if lenght of tokenized not equal max_len , so padding value 0\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "print('padded:', padded[1])\n",
    "print('len padded:', padded.shape)\n",
    "\n",
    "#get attention mask ( 0: not has word, 1: has word)\n",
    "attention_mask = np.where(padded ==0, 0,1)\n",
    "print('attention mask:', attention_mask[1])\n",
    "\n",
    "# Convert input to tensor\n",
    "padded = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "\n",
    "# Train model\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(padded, attention_mask =attention_mask)\n",
    "#     print('last hidden states:', last_hidden_states)\n",
    "\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "print('features:', features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "\n",
    "cl = LogisticRegression()\n",
    "cl.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(cl, 'save_model.pkl')\n",
    "sc = cl.score(X_test, y_test)\n",
    "print('score:', sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45c4f8f21a0eb8a118c781bdd48c3d315da4aac6ad95b6ecc57508b25afb6aa1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
