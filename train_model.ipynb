{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_crawler.csv', delimiter='\\t', header=None)\n",
    "print(df.shape)\n",
    "# get all rows\n",
    "# print(df[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode [101, 1102, 2050, 18699, 2319, 1102, 19098, 2278, 6865, 1010, 2089, 1102, 13699, 1010, 27699, 2080, 6865, 18699, 2319, 2232, 1012, 11503, 2006, 14841, 3211, 1010, 1014, 102]\n",
      "decode [CLS] đa nhan đuoc hang, may đep, giao hang nhanh. cam on tiki, 0 [SEP]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load pretrain model/ tokenizers\n",
    "'''\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#encode lines\n",
    "tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens = True)))\n",
    "print('encode',tokenized[1])\n",
    "# decode\n",
    "print('decode',tokenizer.decode(tokenized[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning model and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: (131,)\n",
      "max len: 138\n",
      "padded: [  101  1102  2050 18699  2319  1102 19098  2278  6865  1010  2089  1102\n",
      " 13699  1010 27699  2080  6865 18699  2319  2232  1012 11503  2006 14841\n",
      "  3211  1010  1014   102     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "len padded: (131, 138)\n",
      "attention mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "features: [[-0.53443784  0.3095479   0.35579208 ...  0.3337525   0.26783708\n",
      "   0.9888859 ]\n",
      " [-0.6813477  -0.2824756   0.16296087 ...  0.25529653  0.13253267\n",
      "   0.9769171 ]\n",
      " [-0.7112863   0.30791676  0.69243515 ...  0.37146655  0.631652\n",
      "   0.7060471 ]\n",
      " ...\n",
      " [-0.69446003 -0.15651524 -0.04057292 ... -0.06758764  0.1692185\n",
      "   0.7198056 ]\n",
      " [-0.7129382  -0.00188149  0.21137421 ...  0.2213878   0.21145663\n",
      "   0.72393197]\n",
      " [-0.6802275  -0.06411637 -0.21788462 ... -0.04009226  0.23876123\n",
      "   0.684275  ]]\n",
      "score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tructt2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#get all label \n",
    "labels = np.zeros(len(df[0]))\n",
    "for i in range(len(df[0])):\n",
    "    labels[i] = df[0][i][-1]\n",
    "print('labels shape:', labels.shape)\n",
    "\n",
    "# get lenght max of tokenized\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print('max len:', max_len)\n",
    "\n",
    "# if lenght of tokenized not equal max_len , so padding value 0\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "print('padded:', padded[1])\n",
    "print('len padded:', padded.shape)\n",
    "\n",
    "#get attention mask ( 0: not has word, 1: has word)\n",
    "attention_mask = np.where(padded ==0, 0,1)\n",
    "print('attention mask:', attention_mask[1])\n",
    "\n",
    "# Convert input to tensor\n",
    "padded = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "\n",
    "# Train model\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(padded, attention_mask =attention_mask)\n",
    "#     print('last hidden states:', last_hidden_states)\n",
    "\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "print('features:', features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "\n",
    "cl = LogisticRegression()\n",
    "cl.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(cl, 'save_model.pkl')\n",
    "sc = cl.score(X_test, y_test)\n",
    "print('score:', sc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
